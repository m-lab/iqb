# Layered Cache Lookup

**Date:** 2025-12-21
**Status:** Implemented (PRs #85--92, v0.5.0)

## Problem

BigQuery queries are the only way to populate the IQB cache, but they
are slow (~minutes) and expensive (~dollars per full run). Team members
and CI/CD environments repeatedly query for the same datasets.

## Decision: Three-Layer Lookup

Cache entries are resolved through a chain of "syncers" tried in order:

1. **Local disk** — check if the file already exists (free, instant).
2. **Remote cache** — download from a manifest-based remote store
   (fast, cheap egress).
3. **BigQuery** — execute the query and write the result (slow, expensive).

The pipeline stops at the first layer that succeeds. This means a
developer who has already run a query once never hits BigQuery again,
and a developer who has never run it only hits BigQuery if the remote
cache doesn't have it either.

## Decision: Protocol-Based Pluggability

The remote cache layer is defined as a Python `Protocol`
(`PipelineRemoteCache`) requiring a single `sync(entry) -> bool`
method. The pipeline and cache packages depend on this Protocol, not
on any concrete implementation.

This was a deliberate choice over an abstract base class: Protocol
uses structural subtyping, so implementations don't need to inherit
from anything. The `ghremote` package implements the Protocol; adding
a different backend (S3, local NFS, etc.) requires no changes to
pipeline code.

## Implementation: `ghremote`

The first implementation used GitHub Releases as the remote store
(hence the name "GitHub Remote", PRs #85--88). This was later
replaced by a GCS bucket (PR #131, see
[2026-01-20-distribution.md](2026-01-20-distribution.md)), and the
package was repurposed as "globally-hosted remote" while keeping the
`ghremote` name for backward compatibility.

The package provides:

- **Manifest** — JSON file listing available files with SHA256 hashes
  and download URLs (format version `v: 0`).
- **IQBRemoteCache** — downloads files from manifest URLs, verifies
  SHA256 during download, writes atomically via `os.replace()`.
- **diff** — compares manifest against local cache to determine what
  needs downloading (used by `iqb cache status` and `iqb cache pull`).

## Atomicity

All writers (pipeline, remote cache, CLI pull) use the same pattern:
create a temporary file in a `TemporaryDirectory` under the target's
parent, then `os.replace()` into the final path. This guarantees
readers never see partial files on Unix. Windows support is not
implemented (YAGNI — the server runs on Linux).

Concurrent writes to the same entry are serialized with advisory file
locks (`filelock` library) on the entry directory.

## The `pipeline` / `cache` Split

The library has two packages that operate on the same on-disk structure
(see [2025-11-24-cache.md](2025-11-24-cache.md)):

- **`pipeline`** is the write side. It executes BigQuery queries,
  streams results to Parquet, and manages the syncer chain described
  above. Because it writes the files, it owns the on-disk format
  specification.

- **`cache`** is the read side. It loads Parquet files, applies
  PyArrow predicate-pushdown filters (country, ASN, city), and
  presents typed data to the calculator. It imports path-construction
  helpers from `pipeline` so the format is defined in one place.

The split exists because writers and readers have different concerns:
the pipeline cares about BigQuery clients, remote sync, locking, and
atomicity; the cache cares about filtering, DataFrame shaping, and
per-provider data models (M-Lab today, Cloudflare/Ookla stubs for
later). Keeping them separate means adding a new data provider only
touches the cache read side, while changing the storage backend only
touches the pipeline write side.

## Lazy Entries

Both packages return lazy objects: `PipelineCacheEntry` on the write
side, `MLabCacheEntry` / `CacheEntry` on the read side. Constructing
an entry is cheap — no I/O happens until the caller explicitly calls
`.sync()` (pipeline) or accesses a DataFrame (cache).

This was deliberate: library users who only need metadata or a subset
of entries should not pay for downloading everything. The expensive
syncer chain (local → remote → BigQuery) only fires for entries that
are actually used. The later `iqb cache pull` CLI command provides an
explicit "download everything" path for users who want a full local
copy, but the library default is to defer work until needed.
