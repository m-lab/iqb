{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bec80ef",
   "metadata": {},
   "source": [
    "# Pipeline Cache\n",
    "\n",
    "Now that we have seen the pipeline, let us see how we can use the remote\n",
    "cache implemented by IQB to avoid re-running queries each time.\n",
    "\n",
    "The source code for them lives inside `./library/src/iqb/ghcache`.\n",
    "\n",
    "As a starting point, let's instantiate the pipeline with:\n",
    "\n",
    "1. a directory where to cache the query results\n",
    "\n",
    "2. a billing project on BigQuery\n",
    "\n",
    "3. the optional remote cache instance.\n",
    "\n",
    "We use the same dataset naming conventions described in the queries chapter, so\n",
    "the cache paths line up with the query templates and granularities.\n",
    "\n",
    "The remote cache assumes the existence of a manifest file describing\n",
    "what remote files are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "652618c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iqb import IQBPipeline, IQBGitHubRemoteCache\n",
    "\n",
    "pipeline = IQBPipeline(\n",
    "    project=\"measurement-lab\",\n",
    "    data_dir=\"02-pipeline-cache.dir\",\n",
    "    remote_cache=IQBGitHubRemoteCache(\n",
    "        data_dir=\"02-pipeline-cache.dir\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6918354e",
   "metadata": {},
   "source": [
    "Before moving forward, let us inspect the content of the manifest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7dea59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"files\": {\n",
      "        \"cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country/data.parquet\": {\n",
      "            \"sha256\": \"82226cc007001bd5545d5b1f036eefe1707c43608581cc5c06e5f055867be376\",\n",
      "            \"url\": \"https://github.com/m-lab/iqb/releases/download/v0.2.0/82226cc00700__cache__v1__20251001T000000Z__20251101T000000Z__downloads_by_country__data.parquet\"\n",
      "        },\n",
      "        \"cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country/stats.json\": {\n",
      "            \"sha256\": \"975ce9997ec33aad693b4367289b130a0ff0258f94d8c904bd8942debc190c3f\",\n",
      "            \"url\": \"https://github.com/m-lab/iqb/releases/download/v0.2.0/975ce9997ec3__cache__v1__20251001T000000Z__20251101T000000Z__downloads_by_country__stats.json\"\n",
      "        },\n",
      "        \"cache/v1/20251001T000000Z/20251101T000000Z/uploads_by_country/data.parquet\": {\n",
      "            \"sha256\": \"c1f384988a07859d42d34d332806de6d8ce576a26d9d42fce6b4c90628b8be90\",\n",
      "            \"url\": \"https://github.com/m-lab/iqb/releases/download/v0.2.0/c1f384988a07__cache__v1__20251001T000000Z__20251101T000000Z__uploads_by_country__data.parquet\"\n",
      "        },\n",
      "        \"cache/v1/20251001T000000Z/20251101T000000Z/uploads_by_country/stats.json\": {\n",
      "            \"sha256\": \"6f49579ca2877e8c90d313b8f3eca94332274a4e209afe90d37e689bfaf6a5e1\",\n",
      "            \"url\": \"https://github.com/m-lab/iqb/releases/download/v0.2.0/6f49579ca287__cache__v1__20251001T000000Z__20251101T000000Z__uploads_by_country__stats.json\"\n",
      "        }\n",
      "    },\n",
      "    \"v\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"02-pipeline-cache.dir/state/ghremote/manifest.json\") as fp:\n",
    "    print(json.dumps(json.load(fp), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c7fd1a",
   "metadata": {},
   "source": [
    "So, as you can see we map paths that must exist in cache with their SHA256\n",
    "and remote location (which currently is GitHub).\n",
    "\n",
    "Now that we understand the content of the manifest, let us create an\n",
    "entry that exists so we can exercise the remote cache.\n",
    "\n",
    "As before, we must create a *lazy* entry first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed58be41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iqb import IQBDatasetGranularity, IQBDatasetMLabTable, iqb_dataset_name_for_mlab\n",
    "\n",
    "entry = pipeline.get_cache_entry(\n",
    "    dataset_name=iqb_dataset_name_for_mlab(\n",
    "        granularity=IQBDatasetGranularity.COUNTRY,\n",
    "        table=IQBDatasetMLabTable.DOWNLOAD,\n",
    "    ),\n",
    "    enable_bigquery=False,  # disable so we know we use the remote cache\n",
    "    start_date=\"2025-10-01\",  # start date is *inclusive*\n",
    "    end_date=\"2025-11-01\",  # end date is *exclusive*\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de61eb",
   "metadata": {},
   "source": [
    "OK, now that we have an entry, we must sync it to get the\n",
    "data we need from the remote cache.\n",
    "\n",
    "\n",
    "Here we're using the most defensive possible code pattern\n",
    "in which we protect against concurrent writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04163ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:filelock:Attempting to acquire lock 128876858280096 on 02-pipeline-cache.dir/cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country/.lock\n",
      "DEBUG:filelock:Lock 128876858280096 acquired on 02-pipeline-cache.dir/cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country/.lock\n",
      "DEBUG:filelock:Attempting to release lock 128876858280096 on 02-pipeline-cache.dir/cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country/.lock\n",
      "DEBUG:filelock:Lock 128876858280096 released on 02-pipeline-cache.dir/cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country/.lock\n"
     ]
    }
   ],
   "source": [
    "with entry.lock():\n",
    "    if not entry.exists():\n",
    "        entry.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b0d94f",
   "metadata": {},
   "source": [
    "Once this is completed, we have synced data inside the cache.\n",
    "\n",
    "Let us print information about the entry and the files it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9dc7b414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02-pipeline-cache.dir/cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country\n",
      "02-pipeline-cache.dir/cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country/data.parquet\n",
      "02-pipeline-cache.dir/cache/v1/20251001T000000Z/20251101T000000Z/downloads_by_country/stats.json\n"
     ]
    }
   ],
   "source": [
    "print(entry.dir_path())\n",
    "print(entry.data_parquet_file_path())\n",
    "print(entry.stats_json_file_path())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bdf6b2",
   "metadata": {},
   "source": [
    "The cache layout matches the local pipeline cache from the previous notebook:\n",
    "`cache/v1/<start>/<end>/<dataset_name>/...`. Here the dataset name encodes\n",
    "the granularity (in this example: country).\n",
    "\n",
    "There is also a stats file. Let's inspect it first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a12945e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"query_start_time\": \"2025-11-27T10:20:54.036274Z\",\n",
      "    \"query_duration_seconds\": 1.311,\n",
      "    \"template_hash\": \"4749dd257891857b70cdab0ea1013e7fbc6e4a06f4e00543a4894c1ba09a5e52\",\n",
      "    \"total_bytes_processed\": 0,\n",
      "    \"total_bytes_billed\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(entry.stats_json_file_path()) as fp:\n",
    "    print(json.dumps(json.load(fp), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29b02b",
   "metadata": {},
   "source": [
    "So, as you can see we record the start time and duration, plus the template\n",
    "hash and byte counts. For more detail on parquet reading and column selection,\n",
    "see the pipeline notebook; here we just confirm the data shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5e7b292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from iqb.pipeline import iqb_parquet_read\n",
    "\n",
    "table = iqb_parquet_read(\n",
    "    entry.data_parquet_file_path(),\n",
    "    country_code=\"US\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd144c",
   "metadata": {},
   "source": [
    "The return value is a `pandas.DataFrame`.\n",
    "\n",
    "Let's see what it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3abb4f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['country_code', 'sample_count', 'download_p1', 'download_p5',\n",
       "       'download_p10', 'download_p25', 'download_p50', 'download_p75',\n",
       "       'download_p90', 'download_p95', 'download_p99', 'latency_p1',\n",
       "       'latency_p5', 'latency_p10', 'latency_p25', 'latency_p50',\n",
       "       'latency_p75', 'latency_p90', 'latency_p95', 'latency_p99',\n",
       "       'loss_p1', 'loss_p5', 'loss_p10', 'loss_p25', 'loss_p50',\n",
       "       'loss_p75', 'loss_p90', 'loss_p95', 'loss_p99'], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e8980d",
   "metadata": {},
   "source": [
    "Let's conclude our overview by just selecting some columns for illustrative purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e86cd2cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_code</th>\n",
       "      <th>sample_count</th>\n",
       "      <th>download_p50</th>\n",
       "      <th>latency_p50</th>\n",
       "      <th>loss_p50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>25430008</td>\n",
       "      <td>121.076161</td>\n",
       "      <td>14.967</td>\n",
       "      <td>0.000314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  country_code  sample_count  download_p50  latency_p50  loss_p50\n",
       "0           US      25430008    121.076161       14.967  0.000314"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[\n",
    "    [\n",
    "        \"country_code\",\n",
    "        \"sample_count\",\n",
    "        \"download_p50\",\n",
    "        \"latency_p50\",\n",
    "        \"loss_p50\",\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7891a3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
